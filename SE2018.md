# SE
# Scottish Digital First Service Standard

![](https://cdn3.iconfinder.com/data/icons/cars-3/512/red_traffic_lights-512.png)

![red][red]
![green][green]
![amber][amber]

[red]: https://github.com/numbat70/SE/redlight.png "Red Light"
[amber]: https://github.com/numbat70/SE/amberlight.png "Amber Light"
[green]: https://github.com/numbat70/SE/greenlight.png "Green Light"

## 1 - User centred   :neutral_face:
`Understand user needs. Research to develop a deep knowledge of who the service users are and what that means for the design of the service.`

### How we get in touch with customers

* We go to trade events and test and research with customers and potential customers (SMAS, Prepare to Export, Amazon Academy, Venturefest )
* We run usability testing, using labs, that test our products and services with customers and potential customers (Taylor McKenzie)
* We perform online 1 to 1 testing with customers (Skype)
* We perform online unmoderated testing with potential customers (Whatusersdo)
* We observe session recording, heat-mapping and analytics, to observe common user behaviour patterns
* We also periodically ask customers for feedback on our site when certain Frustration or Completion triggers occur 
* We develop our services against user needs and refine these into user stories to work with.
* _**Tress**_


## 2 - Usable and accessible :neutral_face:
`Create a service that is usable, accessible and intuitive enough that users succeed first time.`

* We perform regular Expert Reviews of UX, usability and accessibility
* We have run workshops where team members try to access our digital services whilst replicating sight loss. This gives them a very real insight to how hard it can be for different user groups. 
* We run education session for team members and partners on how to avoid or overcome these issues. 
* We watch usability testing videos, from online unmoderated testing , as a team and review against usability and accessibility
* We watch usability testing videos, from 1 to 1 testing,  as a team and review against usability and accessibility
* _**David G (UX), Tress, Daren**_


## 3 - Channel shift
`Identify and, wherever possible, remove impediments that prevent users from using the digital service, clearly establishing it as the primary channel. Plan to provide appropriate assisted digital support if necessary.`

* We are designing services where Digital First is always considered for the context of that service...

* _**Lindsay**_


## 4 - Consistent user experience
`Build a service consistent with the user experience of the rest of mygov.scot including using the design patterns and style guide.`

* We are re-using common design patterns as they develop
* We are adhering to common style, tone and coding guides
* _**David G (UX)**_

## 5 - Continuous feedback
`Put a plan in place for ongoing user research and usability testing to continuously seek feedback and input from users to improve the service.`

* We probably need to think harder about this one than we think. We are probably a bit light.
* _**Tress**_

## 6 - Data driven
`Use tools for analysis that collect performance data. Use this data to analyse the success of the service and to translate this into features and tasks for the next phase of development.`

* Analytics
* Heatmaps
* Dashboard
* Session recordings
* Customer Research stats
* _**Glen or new analytics team lead**_


## 7 - Cross-functional team
`Put in place a sustainable multidisciplinary team that can design, build and operate the service, led by a suitably skilled senior manager with decision-making responsibility.`

* Yes. We have a cross functional team
* No. It is not fully sustainable due to resource constraints
* _**Linsdsay**_


## 8 - Sustainability
`Build a service that can be iterated and improved on a frequent basis and make sure that you have the capability, resources and technical flexibility to do so.`

* **Nope**
* Not  enough resources
* Not enough technical flexibility
* Capability is fine but stretched
* _**Paul Hanton & Ross**_


## 9 - Continuous improvement
`Build the service incrementally, releasing early and often, using the iterative and user-centred methods set out in the GDS service manual.`

* Yes in usual practice
* We have relaunched an entire site due to time constraints
* Limited by resources * Time
* _**Lindsay & Ross**_

## 10 - Business continuity
`Define, document and regularly test a plan to handle disasters and other incidents that may cause the digital service to be taken temporarily offline.`

* **Unsure**
* Reliant on core platform capabilities. 
* Not yet fully reliant on Digital applications as they mainly channel into email or CRM.  
* _**Paul Hanton**_

## 11 - Technology appraisal
`Evaluate what technology, tools and systems will be used to build, host, operate and measure the service, and how to procure them.`

* **Discuss with Technical Architect, Analytics and Assurance teams**
* _**Paul Hanton**_

## 12 - Information governance

`Evaluate what user data and information the digital service will be providing or storing, and address the security level, legal responsibilities, privacy issues and risks associated with the service (consulting with experts where appropriate).`

* **Unsure**
* _**Paul Hanton**_


## 13 - Open data
`Make all non-personal, non-commercially sensitive data from the service available for re-use by others under an appropriate licence.`

* **Unsure**
* _**Paul Hanton**_


## 14 - Ecosystem
`Identify how your service aligns with Scotlandâ€™s digital ecosystem.`

* **Unsure**
* _**Paul Hanton**_


## 15 - Open source
`Make all new source code open and reusable, and publish it under appropriate licences (or provide a convincing explanation as to why this cannot be done for specific subsets of the source code).`

* Fail. We are using a .NET stack. 
* _**Paul Hanton**_


## 16 - Open standards
`Use open standards and common government platforms where available.`

* **Unsure**
* _**Paul Hanton**_


## 17 - Green ICT
`Deliver a digital service whose impact on the environment, over its whole lifecycle, is understood. Plan to reduce the environmental impact of the service over time.`

* **Unsure**
* This is probably part of the hosting contract. 
* _**Paul Hanton**_


## 18 - Data hosting and data centres
`Adopt cloud computing or virtualisation as the preferred approaches to the delivery of data hosting for the service.`

* Pass (AWS)
* This is managed at a platform level. Reference this. 
* _**Paul Hanton**_


## 19 - Performance management
`Identify performance indicators for the service, including the 4 mandatory key performance indicators (KPIs) defined in the GDS service manual. Establish a benchmark for each metric and make a plan to enable improvements.`

* Not yet measuring 1: Cost per transaction 
* Not yet measuring 2. User Satisfaction
* We are partially measuring completion rate (KPI 3)
* We are currently measuring digital take-up (KPI 4)
* _**Analytics Team**_


## 20 - Transparent
`Publish performance data on the Digital First Performance Platform.`

* **Fail**
* _**Hazel or Carrie**_


## 21 - Operational acceptance
`Regularly test the end-to-end service in an environment identical to that of the live version, including on all common browsers and devices, and using dummy accounts and a representative sample of users.`

* **Unsure**
* Daren can give information about this
* _**Daren**_


## 22 - Sponsor acceptance
`Test the service from beginning to end with the sponsor or minister responsible for it.`

* **Unsure*
* _**Siobhan**_

